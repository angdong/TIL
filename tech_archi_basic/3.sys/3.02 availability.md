# 3.2 가용성
- [3.2 가용성](#32-가용성)
  - [가용성 구성 방안](#가용성-구성-방안)
  - [구성 요소 이중화](#구성-요소-이중화)
    - [네트워크 이중화](#네트워크-이중화)
    - [스토리지 채널 이중화](#스토리지-채널-이중화)
    - [클러스터링](#클러스터링)
    - [DBMS A-A 클러스터 구성](#dbms-a-a-클러스터-구성)
    - [L4 스위치 로드밸런싱](#l4-스위치-로드밸런싱)
  - [스토리지 복제](#스토리지-복제)
    - [동기식 데이터 복제](#동기식-데이터-복제)
    - [비동기식 데이터 복제](#비동기식-데이터-복제)
  - [사이트 가용성](#사이트-가용성)
  - [Cloud의 이중화 구성](#cloud의-이중화-구성)


가용성

생산적인 목적으로의 시간 사용 비율\
컴퓨터 시스템 -> 서비스가 가능한 기간

고가용성

장애 최소화하도록 관리하거나 예상된 다운타임을 최소화함으로써 서비스 손실 줄일 수 있도록 하는 행위

## 가용성 구성 방안
서비스 지속을 위한 방안

1. 주요 부품 이중화(시스템 단위)
2. 서버 이중화(업무 단위)
3. site 이중화(센터 단위)

## 구성 요소 이중화

### 네트워크 이중화
Linux Bonding
* 가상의 NIC와 실제 NIC가 동일하게 조회됨

Windows Teaming

### 스토리지 채널 이중화
스토리지 FA Port, SAN 스위치, HBA등을 다중화 구성하여 확보

* Linux Multipath
  * 스토리지 채널 이중화 위해 2개 이상의 채널을 서버와 연결하면 서버에서는 각 채널별로 다른 디스크로 인식됨
  * 이를 **Multipath가 가상 디바이스로 묶어서 이중화**를 구현

* Windows MPIO
  * MPIO 설치 전에는 외장 디스크가 2개로 보이다가 설치 후에는 1개로 보임

### 클러스터링
서버를 이중화하는 방법

2대 이상의 독립된 시스템들이 모여서 각 자원을 공유 및 활용하여, 한 시스템의 장애 발생 시에도 최소의 시간으로 서비스를 지속하기 위해서 나타난 개념

방법
1. Fail-Over 혹은 HA 클러스터링
2. 로드밸런싱
3. 공유 프로세싱

* HA 클러스터 구성
  * Active 서버에서 공유 디스크 상에 파일을 마운트
  * 장애 발생시 Standby 서버에서 파일을 마운트해서 사용
  * RHEL pacemaker(리눅스)
    * 하나의 서비스를 이루는 "리소스"를 모아 "리소스그룹"으로 만들어서 fail-over(장애 극복) 하는 개념
    * fencing network
      * 네트워크 단절된 노드에 panic 발생시켜 $^*$split brain 예방하는 구조
  * Windows 장애조치 클러스터
    * 정보들을 '역할'로 정의하여 노드 간 fail-over 시킴
    * 역할이 올라가 있는 서버가 다운 -> 역할에 들어 있는 ip, 스토리지, 서비스는 다른 노드로 함께 넘어감
    * Quorum
      * 쿼럼 설정시 절반의 노드가 다운되어도 클러스터 중단X
      * split brain 발생시 쿼럼을 가지지 못한 노드가 다운

> $^*$**split brain** 네트워크 파티셔닝 장애로 인해 시스템이 Sub-Cluster로 쪼개짐에 따라 각 Sub-Cluster가 스스로를 Primary 또는 정상적인 서비스라고 인식하는 것을 의미

### DBMS A-A 클러스터 구성
대표적인 벤더의 A-A(Active-Active) 구성은 Shared Disk Concurrent Write 기반으로 구성

### L4 스위치 로드밸런싱
웹 서버의 경우 일반적으로 L4 스위치를 이용하여 고가용성 시스템 구현

방식
1. Round Robin
2. Least Connection\
   open session이 가장 적은 서버와 세션 맺음
3. Response time\
   응답시간이 빠른 쪽으로 많은 세션 보냄
4. Hash\
   source IP를 기반으로 서버 선택

* SLB Proxy Mode
  * Response Packet이 L4를 거쳐서 Client에게 전달(L4 부하 가중)
  * L4와 서버는 다른 네트워크 대역에 있어도 무방
* DSR
  * direct server return
  * Response Packet이 서버에서 바로 client에게 전달(L4 부하 경감)
  * L4와 서버가 같은 Network 대역에 있어야 함
  * 서버의 Loopback Adapter에 L4 VIP추가 필요

## 스토리지 복제
스토리지 복제를 통한 데이터 가용성 확보

원본 볼륨과 복제본 볼륨이 같도록 구성을 잘 해야 함
### 동기식 데이터 복제
재해시 데이터 손실 최소화\
스토리지 성능 일부 영향\
스토리지/센터 거리 제약 존재

### 비동기식 데이터 복제
재해시 데이터 일부 손실 감수\
스토리지 성능 영향 최소화

## 사이트 가용성
재해복구센터

구성방식
1. Active-Standby
   * 평상시, DR시스템 자원은 Standby 상태
   * AP, DB데이터는 지속적으로 복제
   * 운영 방안 단순하고, 복구시간 오래 걸림
2. Active-Active(AP only)
   * 평상시, 지리적 환경이 다른 두 곳의 센터에서 업무서비스(AP only)를 제공
   * DB는 복제되는 standby
   * 센터간 거리에 따라 성능 저하 가능성
   * 추가 비용 발생
3. Active-Active
   * 평상시에 주센터, DR센터의 AP, DB서버 모두 업무 서비스 참여
   * DR 전환 절차 필요 없음
   * DB 노드 수 및 거리에 따라 성능 저하 발생
   * 추가 비용 발생

## Cloud의 이중화 구성

* 확장성 고려하여 특정 서버 종속을 최소화하여 설계

서버의 수평적 확장시 가장 중요하게 고려해야 하는 것이 stateless 요건(**status를 외부에 저장해야 함**)

* 분산 DB, DB복제

고사양 단일 DB를 공유하는 형태 $\to$ 다수개의 DB로 분산, 복제기반으로 변화

Active-Active 이중화 구조는 Master-Slave 방식으로 변경 필요

1. Active-Active(Shared DB)
   * 다중 노드에서 R/W I/O 동시 처리
   * Active-Active 구조
2. Master-Salve(Replication DB)
   * Master/Slave 구조
   * Master(write)는 1개 Slave는 다중노드 가능
   * Slave DB를 standby, read I/O 분산용도로 사용

기타

1. 인스턴스 이중화: ELB 사용하여 부하 분산 이중화
2. RDS 이중화: DB Replica 통한 이중화
3. Multi-AZ 이중화: Site 장애에 대응 가능